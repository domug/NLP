{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset\n",
    "- Let's import preprocessed movie review dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T06:45:45.199127Z",
     "start_time": "2021-01-09T06:45:44.369131Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = \"Preprocessed/\"     # Directory path where data is located\n",
    "TRAIN_CLEAN_DATA = \"clean_train_df.csv\"\n",
    "\n",
    "train_data = pd.read_csv(DATA_PATH + TRAIN_CLEAN_DATA)\n",
    "reviews = list(train_data.review)  # preprocessed reviews\n",
    "sentiments = list(train_data.sentiment) # corresponding sentiment label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "\n",
    "- In order to be fitted to models, our text data have to be transformed into a vector. \n",
    "- We are going to use three different vectorization technics - **\"TF-IDF Vectorization\"**, **\"Count Vertorization\"** and **\"word2vec Vectorization\"**.\n",
    "- Let's define each vectorizer and compare their performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "- TF-IDF stands for Term Frequency - Inverse Document Frequency.\n",
    "- Basically, TF-IDF value gets larger if a specific word appears frequently in a specific document only. Therefore, commonly used words like pronouns get small values.\n",
    "- We will use scikitlearn's `TfidfVectorizer` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T06:45:52.825530Z",
     "start_time": "2021-01-09T06:45:45.201280Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define vectorizer instance\n",
    "vectorizer = TfidfVectorizer(min_df = 0, analyzer = \"word\", sublinear_tf = True,\n",
    "                            ngram_range = (1,2), max_features = 500)\n",
    "\n",
    "# Transform movie reviews\n",
    "X_TFIDF = vectorizer.fit_transform(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanations for the arguments used above in `TfidfVectorizer`:\n",
    "\n",
    "- `min_df`:   To ignore the terms that have a **document frequency** strictly lower than the given threshold\n",
    "- `analyzer`:   Whether the feature should be made of word or character n-grams (\"word\" / \"char\" / \"word_wb\")\n",
    "- `sublinear_tf`:   Apply sublinear tf (term frequency) scaling, i.e. replace tf with 1 + log(tf). It is used to deal with outliers in tf\n",
    "- `ngram_range`: The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used. \n",
    "\n",
    "---\n",
    "\n",
    "For additional information, check out the [official website](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) or [this site](https://chan-lab.tistory.com/27) (in Korean)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorizer\n",
    "\n",
    "- count vectorizer extracts features of text data by relative frequency of each words. \n",
    "- Although this method is very simple and easy to implement, it might be less practical because the frequently used but less meaningful words like pronouns can have large values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T06:45:55.145775Z",
     "start_time": "2021-01-09T06:45:52.827851Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=\"word\", max_features=500)\n",
    "X_countvect = vectorizer.fit_transform(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyperparameters are very similar to those of TF-IDF\n",
    "\n",
    "- `analyzer`: unit of analysis\n",
    "- `max_features`: maximum number of words to consider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec\n",
    "\n",
    "- word2vec is a prediction-based word representation method. \n",
    "- It consists of two famous models which are **CBOW (Continuous Bag of Words)** and **Skip-Gram**, \n",
    "    - CBOW: speculates a specific word by using nearby words\n",
    "    - Skip-Gram: speculates nearby words by a specific word\n",
    "- word2vec models are known to be **able to catch complicated features** of human language including relationships among words.\n",
    "- On average it is said that Skip-Gram has better performance over CBOW\n",
    "\n",
    "---\n",
    "\n",
    "- Unlike TF-IDF, word2vec vectorizer takes a **word-separated list** as an input.\n",
    "- Each word is transformed into a n-dimensional vector designated by `size` parameter.\n",
    "- Thus, if a review consists of m words, then the output is a (m x n) dimensional matrix.\n",
    "\n",
    "---\n",
    "\n",
    "https://wikidocs.net/22660"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T06:45:55.538235Z",
     "start_time": "2021-01-09T06:45:55.147912Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define input data\n",
    "sentences = [review.split() for review in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T06:45:55.544487Z",
     "start_time": "2021-01-09T06:45:55.540272Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters of word2vec model\n",
    "num_features = 500    # dimension of each embedded vectors\n",
    "min_word_count = 35   # words with word count less than the set value are ignored\n",
    "num_workers = 8       # number of processers\n",
    "context = 10          # Set context window size (similar to n-gram)\n",
    "downsampling = 1e-3   # downsampling rate for correct words to increase speed. 0.001 is used generally.\n",
    "sg = 1                # 0 for CBOW, 1 for Skip-gram\n",
    "\n",
    "# For Checking progression details\n",
    "import logging\n",
    "logging.basicConfig(format = \"%(asctime)s : %(levelname)s : %(message)s\", level = logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T06:46:59.114713Z",
     "start_time": "2021-01-09T06:45:55.546382Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-09 15:45:55,784 : INFO : collecting all words and their counts\n",
      "2021-01-09 15:45:55,785 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-09 15:45:56,000 : INFO : PROGRESS: at sentence #10000, processed 1205223 words, keeping 51374 word types\n",
      "2021-01-09 15:45:56,202 : INFO : PROGRESS: at sentence #20000, processed 2396605 words, keeping 67660 word types\n",
      "2021-01-09 15:45:56,310 : INFO : collected 74065 word types from a corpus of 2988089 raw words and 25000 sentences\n",
      "2021-01-09 15:45:56,311 : INFO : Loading a fresh vocabulary\n",
      "2021-01-09 15:45:56,360 : INFO : effective_min_count=35 retains 8973 unique words (12% of original 74065, drops 65092)\n",
      "2021-01-09 15:45:56,360 : INFO : effective_min_count=35 leaves 2657397 word corpus (88% of original 2988089, drops 330692)\n",
      "2021-01-09 15:45:56,385 : INFO : deleting the raw counts dictionary of 74065 items\n",
      "2021-01-09 15:45:56,388 : INFO : sample=0.001 downsamples 28 most-common words\n",
      "2021-01-09 15:45:56,388 : INFO : downsampling leaves estimated 2526274 word corpus (95.1% of prior 2657397)\n",
      "2021-01-09 15:45:56,404 : INFO : estimated required memory for 8973 words and 500 dimensions: 40378500 bytes\n",
      "2021-01-09 15:45:56,405 : INFO : resetting layer weights\n",
      "2021-01-09 15:45:58,036 : INFO : training model with 8 workers on 8973 vocabulary and 500 features, using sg=1 hs=0 sample=0.001 negative=5 window=10\n",
      "2021-01-09 15:45:59,220 : INFO : EPOCH 1 - PROGRESS: at 8.24% examples, 177207 words/s, in_qsize 15, out_qsize 0\n",
      "2021-01-09 15:46:00,239 : INFO : EPOCH 1 - PROGRESS: at 18.25% examples, 213028 words/s, in_qsize 15, out_qsize 0\n",
      "2021-01-09 15:46:01,294 : INFO : EPOCH 1 - PROGRESS: at 26.57% examples, 208393 words/s, in_qsize 15, out_qsize 0\n",
      "2021-01-09 15:46:02,420 : INFO : EPOCH 1 - PROGRESS: at 37.01% examples, 216106 words/s, in_qsize 16, out_qsize 0\n",
      "2021-01-09 15:46:03,430 : INFO : EPOCH 1 - PROGRESS: at 46.47% examples, 219204 words/s, in_qsize 15, out_qsize 0\n",
      "2021-01-09 15:46:04,433 : INFO : EPOCH 1 - PROGRESS: at 54.90% examples, 218946 words/s, in_qsize 15, out_qsize 0\n",
      "2021-01-09 15:46:05,559 : INFO : EPOCH 1 - PROGRESS: at 63.66% examples, 215230 words/s, in_qsize 15, out_qsize 0\n",
      "2021-01-09 15:46:06,650 : INFO : EPOCH 1 - PROGRESS: at 71.99% examples, 212306 words/s, in_qsize 15, out_qsize 0\n",
      "2021-01-09 15:46:07,663 : INFO : EPOCH 1 - PROGRESS: at 81.75% examples, 215237 words/s, in_qsize 15, out_qsize 0\n",
      "2021-01-09 15:46:08,795 : INFO : EPOCH 1 - PROGRESS: at 90.68% examples, 213663 words/s, in_qsize 15, out_qsize 0\n",
      "2021-01-09 15:46:09,570 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-01-09 15:46:09,608 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-01-09 15:46:09,637 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-01-09 15:46:09,665 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-01-09 15:46:09,758 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-01-09 15:46:09,764 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-09 15:46:09,792 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-09 15:46:09,806 : INFO : EPOCH 1 - PROGRESS: at 100.00% examples, 214686 words/s, in_qsize 0, out_qsize 1\n",
      "2021-01-09 15:46:09,807 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-09 15:46:09,807 : INFO : EPOCH - 1 : training on 2988089 raw words (2525962 effective words) took 11.8s, 214666 effective words/s\n",
      "2021-01-09 15:46:10,987 : INFO : EPOCH 2 - PROGRESS: at 8.24% examples, 178762 words/s, in_qsize 15, out_qsize 0\n",
      "2021-01-09 15:46:11,999 : INFO : EPOCH 2 - PROGRESS: at 17.26% examples, 203134 words/s, in_qsize 15, out_qsize 0\n",
      "2021-01-09 15:46:13,013 : INFO : EPOCH 2 - PROGRESS: at 26.25% examples, 209553 words/s, in_qsize 15, out_qsize 0\n",
      "2021-01-09 15:46:14,119 : INFO : EPOCH 2 - PROGRESS: at 34.47% examples, 204401 words/s, in_qsize 15, out_qsize 0\n",
      "2021-01-09 15:46:15,131 : INFO : EPOCH 2 - PROGRESS: at 43.10% examples, 206581 words/s, in_qsize 15, out_qsize 0\n",
      "2021-01-09 15:46:16,151 : INFO : EPOCH 2 - PROGRESS: at 52.35% examples, 210433 words/s, in_qsize 15, out_qsize 0\n",
      "2021-01-09 15:46:17,226 : INFO : EPOCH 2 - PROGRESS: at 60.96% examples, 209350 words/s, in_qsize 15, out_qsize 0\n",
      "2021-01-09 15:46:18,247 : INFO : EPOCH 2 - PROGRESS: at 70.29% examples, 211919 words/s, in_qsize 15, out_qsize 0\n",
      "2021-01-09 15:46:19,251 : INFO : EPOCH 2 - PROGRESS: at 79.41% examples, 213380 words/s, in_qsize 15, out_qsize 0\n",
      "2021-01-09 15:46:20,286 : INFO : EPOCH 2 - PROGRESS: at 87.51% examples, 212277 words/s, in_qsize 15, out_qsize 0\n",
      "2021-01-09 15:46:21,342 : INFO : EPOCH 2 - PROGRESS: at 96.76% examples, 212389 words/s, in_qsize 10, out_qsize 0\n",
      "2021-01-09 15:46:21,418 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-01-09 15:46:21,467 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-01-09 15:46:21,523 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-01-09 15:46:21,532 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-01-09 15:46:21,567 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-01-09 15:46:21,577 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-09 15:46:21,624 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-09 15:46:21,661 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-09 15:46:21,662 : INFO : EPOCH - 2 : training on 2988089 raw words (2526084 effective words) took 11.8s, 213246 effective words/s\n",
      "2021-01-09 15:46:22,897 : INFO : EPOCH 3 - PROGRESS: at 8.24% examples, 170012 words/s, in_qsize 16, out_qsize 0\n",
      "2021-01-09 15:46:23,931 : INFO : EPOCH 3 - PROGRESS: at 15.95% examples, 180930 words/s, in_qsize 15, out_qsize 0\n",
      "2021-01-09 15:46:24,942 : INFO : EPOCH 3 - PROGRESS: at 25.94% examples, 201925 words/s, in_qsize 15, out_qsize 0\n",
      "2021-01-09 15:46:26,036 : INFO : EPOCH 3 - PROGRESS: at 34.47% examples, 201256 words/s, in_qsize 16, out_qsize 0\n",
      "2021-01-09 15:46:27,037 : INFO : EPOCH 3 - PROGRESS: at 43.72% examples, 207511 words/s, in_qsize 15, out_qsize 0\n",
      "2021-01-09 15:46:28,052 : INFO : EPOCH 3 - PROGRESS: at 52.60% examples, 210027 words/s, in_qsize 15, out_qsize 0\n",
      "2021-01-09 15:46:29,140 : INFO : EPOCH 3 - PROGRESS: at 60.96% examples, 207516 words/s, in_qsize 15, out_qsize 0\n",
      "2021-01-09 15:46:30,143 : INFO : EPOCH 3 - PROGRESS: at 69.97% examples, 209753 words/s, in_qsize 15, out_qsize 0\n",
      "2021-01-09 15:46:31,170 : INFO : EPOCH 3 - PROGRESS: at 78.70% examples, 210021 words/s, in_qsize 15, out_qsize 0\n",
      "2021-01-09 15:46:32,178 : INFO : EPOCH 3 - PROGRESS: at 86.86% examples, 209816 words/s, in_qsize 15, out_qsize 0\n",
      "2021-01-09 15:46:33,186 : INFO : EPOCH 3 - PROGRESS: at 95.78% examples, 210328 words/s, in_qsize 13, out_qsize 0\n",
      "2021-01-09 15:46:33,440 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-01-09 15:46:33,459 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-01-09 15:46:33,483 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-01-09 15:46:33,504 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-01-09 15:46:33,509 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-01-09 15:46:33,535 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-09 15:46:33,569 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-09 15:46:33,621 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-09 15:46:33,622 : INFO : EPOCH - 3 : training on 2988089 raw words (2525953 effective words) took 12.0s, 211283 effective words/s\n",
      "2021-01-09 15:46:34,903 : INFO : EPOCH 4 - PROGRESS: at 8.24% examples, 163883 words/s, in_qsize 15, out_qsize 0\n",
      "2021-01-09 15:46:35,958 : INFO : EPOCH 4 - PROGRESS: at 15.95% examples, 175800 words/s, in_qsize 15, out_qsize 0\n",
      "2021-01-09 15:46:37,152 : INFO : EPOCH 4 - PROGRESS: at 26.57% examples, 192422 words/s, in_qsize 15, out_qsize 0\n",
      "2021-01-09 15:46:38,159 : INFO : EPOCH 4 - PROGRESS: at 36.36% examples, 205167 words/s, in_qsize 15, out_qsize 0\n",
      "2021-01-09 15:46:39,318 : INFO : EPOCH 4 - PROGRESS: at 45.16% examples, 201722 words/s, in_qsize 15, out_qsize 0\n",
      "2021-01-09 15:46:40,325 : INFO : EPOCH 4 - PROGRESS: at 53.26% examples, 202729 words/s, in_qsize 15, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-09 15:46:41,334 : INFO : EPOCH 4 - PROGRESS: at 62.64% examples, 206763 words/s, in_qsize 15, out_qsize 0\n",
      "2021-01-09 15:46:42,491 : INFO : EPOCH 4 - PROGRESS: at 71.62% examples, 205336 words/s, in_qsize 15, out_qsize 0\n",
      "2021-01-09 15:46:43,520 : INFO : EPOCH 4 - PROGRESS: at 80.79% examples, 206853 words/s, in_qsize 15, out_qsize 0\n",
      "2021-01-09 15:46:44,530 : INFO : EPOCH 4 - PROGRESS: at 88.98% examples, 206941 words/s, in_qsize 14, out_qsize 1\n",
      "2021-01-09 15:46:45,526 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-01-09 15:46:45,545 : INFO : EPOCH 4 - PROGRESS: at 98.12% examples, 208209 words/s, in_qsize 6, out_qsize 1\n",
      "2021-01-09 15:46:45,549 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-01-09 15:46:45,630 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-01-09 15:46:45,682 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-01-09 15:46:45,725 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-01-09 15:46:45,732 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-09 15:46:45,764 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-09 15:46:45,789 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-09 15:46:45,789 : INFO : EPOCH - 4 : training on 2988089 raw words (2526554 effective words) took 12.2s, 207719 effective words/s\n",
      "2021-01-09 15:46:46,794 : INFO : EPOCH 5 - PROGRESS: at 7.88% examples, 200797 words/s, in_qsize 15, out_qsize 0\n",
      "2021-01-09 15:46:47,965 : INFO : EPOCH 5 - PROGRESS: at 15.95% examples, 188746 words/s, in_qsize 15, out_qsize 0\n",
      "2021-01-09 15:46:48,977 : INFO : EPOCH 5 - PROGRESS: at 24.98% examples, 199924 words/s, in_qsize 15, out_qsize 0\n",
      "2021-01-09 15:46:49,989 : INFO : EPOCH 5 - PROGRESS: at 32.49% examples, 197714 words/s, in_qsize 15, out_qsize 0\n",
      "2021-01-09 15:46:51,012 : INFO : EPOCH 5 - PROGRESS: at 39.85% examples, 194305 words/s, in_qsize 16, out_qsize 0\n",
      "2021-01-09 15:46:52,134 : INFO : EPOCH 5 - PROGRESS: at 47.78% examples, 191715 words/s, in_qsize 15, out_qsize 0\n",
      "2021-01-09 15:46:53,176 : INFO : EPOCH 5 - PROGRESS: at 55.57% examples, 191941 words/s, in_qsize 16, out_qsize 0\n",
      "2021-01-09 15:46:54,242 : INFO : EPOCH 5 - PROGRESS: at 63.66% examples, 191591 words/s, in_qsize 16, out_qsize 0\n",
      "2021-01-09 15:46:55,455 : INFO : EPOCH 5 - PROGRESS: at 71.62% examples, 188400 words/s, in_qsize 15, out_qsize 0\n",
      "2021-01-09 15:46:56,568 : INFO : EPOCH 5 - PROGRESS: at 79.77% examples, 187612 words/s, in_qsize 15, out_qsize 0\n",
      "2021-01-09 15:46:57,590 : INFO : EPOCH 5 - PROGRESS: at 87.51% examples, 188406 words/s, in_qsize 16, out_qsize 0\n",
      "2021-01-09 15:46:58,664 : INFO : EPOCH 5 - PROGRESS: at 95.78% examples, 188256 words/s, in_qsize 12, out_qsize 1\n",
      "2021-01-09 15:46:58,854 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-01-09 15:46:58,868 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-01-09 15:46:58,950 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-01-09 15:46:58,991 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-01-09 15:46:58,999 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-01-09 15:46:59,002 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-09 15:46:59,006 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-09 15:46:59,111 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-09 15:46:59,112 : INFO : EPOCH - 5 : training on 2988089 raw words (2526117 effective words) took 13.3s, 189673 effective words/s\n",
      "2021-01-09 15:46:59,113 : INFO : training on a 14940445 raw words (12630670 effective words) took 61.1s, 206804 effective words/s\n"
     ]
    }
   ],
   "source": [
    "# Define and train word2vec Skip-gram model\n",
    "from gensim.models import word2vec\n",
    "\n",
    "print(\"Training Model...\")\n",
    "model = word2vec.Word2Vec(sentences,\n",
    "                         workers = num_workers,\n",
    "                         size = num_features,\n",
    "                         min_count = min_word_count,\n",
    "                         window = context,\n",
    "                         sample = downsampling,\n",
    "                         sg = sg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Trained word2vec models can be saved and reused in the future.\n",
    "- It is a good practice to include information about hyperparameters in the model's name.\n",
    "- Once the model is saved, it can be reused by `Word2Vec.load()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T06:46:59.471676Z",
     "start_time": "2021-01-09T06:46:59.116761Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-09 15:46:59,117 : INFO : saving Word2Vec object under 500features_35minwords_10context_Skipgram, separately None\n",
      "2021-01-09 15:46:59,119 : INFO : not storing attribute vectors_norm\n",
      "2021-01-09 15:46:59,119 : INFO : not storing attribute cum_table\n",
      "2021-01-09 15:46:59,469 : INFO : saved 500features_35minwords_10context_Skipgram\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "model_name = \"500features_35minwords_10context_Skipgram\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since the number of words inside each reviews are all different, we have to standardize them.\n",
    "- A simple way to do that is to use average vector as a representative of a review (**feature vector**).\n",
    "- We will make a function to calculate this feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T06:46:59.477948Z",
     "start_time": "2021-01-09T06:46:59.473349Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define function to calculate feature vectors\n",
    "import numpy as np\n",
    "\n",
    "def get_features(words, model, num_features):\n",
    "    \"\"\"\n",
    "    Function to calculate mean vector of all embedded vectors.\n",
    "    Calculated mean vector will be used to represent a single review. (Feature Vector)\n",
    "    \n",
    "    words: a single review consisting of embedded vectors\n",
    "    model: pretrained word2vec model\n",
    "    num_features: dimension of embedded vectors (Same as num_features in word2vec model definition)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize output vector\n",
    "    feature_vector = np.zeros((num_features), dtype=np.float32)\n",
    "    num_words = 0   # total count of valid words inside a review\n",
    "    \n",
    "    # word dictionary\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    # Calculate mean vector\n",
    "    for w in words:\n",
    "        if w in index2word_set:\n",
    "            num_words += 1\n",
    "            feature_vector = np.add(feature_vector, model[w])\n",
    "            \n",
    "    feature_vector = np.divide(feature_vector, num_words)\n",
    "    \n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As a final step, let's define function to get averaged feature vectors for the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T06:46:59.483670Z",
     "start_time": "2021-01-09T06:46:59.480088Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataset(reviews, model, num_features):\n",
    "    \"\"\"\n",
    "    Function to get feature vectors for all the reviews.\n",
    "    A list inside which all feature vectors are stacked is returned.\n",
    "    \n",
    "    reviews: entire dataset\n",
    "    model: pretrained word2vec model\n",
    "    num_features: dimension of embedded vectors (Same as num_features in word2vec model definition)\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset = [get_features(review, model, num_features) for review in reviews]\n",
    "    \n",
    "    reviewsFeatureVecs = np.stack(dataset)\n",
    "    \n",
    "    return reviewsFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T06:47:20.871181Z",
     "start_time": "2021-01-09T06:46:59.485658Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-f55b9ad5d5c0>:25: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  feature_vector = np.add(feature_vector, model[w])\n"
     ]
    }
   ],
   "source": [
    "# Train input dataset to be used for fitting the model\n",
    "X_word2vec = get_dataset(sentences, model, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define and train model\n",
    "\n",
    "- We are going to try several models and compare their performances.\n",
    "    - Logistic Regression\n",
    "    - Random Forest\n",
    "    - RNN\n",
    "    - CNN\n",
    "- We use n-fold cross validation to evaluate each model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T06:47:20.962601Z",
     "start_time": "2021-01-09T06:47:20.872681Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Define model\n",
    "lgs = LogisticRegression(class_weight=\"balanced\")\n",
    "\n",
    "# Train labels\n",
    "y = sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `class_weight`: by using \"balanced\" mode, each labels are trained fairly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TF-IDF inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T06:47:21.589919Z",
     "start_time": "2021-01-09T06:47:20.964492Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean F1 Score: 0.838\n",
      "Mean Precision Score: 0.828\n",
      "Mean Recall Score: 0.849\n",
      "Mean Accuracy Score: 0.836\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Evaluate model by 5 fold cross validation\n",
    "scoring = ['f1','precision','recall', \"accuracy\"]    # metrics to be evaluated\n",
    "lgs_scores = cross_validate(lgs, X_TFIDF, y, scoring=scoring, cv=5, return_train_score=False)\n",
    "\n",
    "print(\"Mean F1 Score: {:.3f}\".format(np.mean(lgs_scores[\"test_f1\"])))\n",
    "print(\"Mean Precision Score: {:.3f}\".format(np.mean(lgs_scores[\"test_precision\"])))\n",
    "print(\"Mean Recall Score: {:.3f}\".format(np.mean(lgs_scores[\"test_recall\"])))\n",
    "print(\"Mean Accuracy Score: {:.3f}\".format(np.mean(lgs_scores[\"test_accuracy\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Count vectorizer inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T06:47:23.111962Z",
     "start_time": "2021-01-09T06:47:21.591780Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean F1 Score: 0.837\n",
      "Mean Precision Score: 0.825\n",
      "Mean Recall Score: 0.849\n",
      "Mean Accuracy Score: 0.835\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model by 5 fold cross validation\n",
    "scoring = ['f1','precision','recall', \"accuracy\"]    # metrics to be evaluated\n",
    "lgs_scores = cross_validate(lgs, X_countvect, y, scoring=scoring, cv=5, return_train_score=False)\n",
    "\n",
    "print(\"Mean F1 Score: {:.3f}\".format(np.mean(lgs_scores[\"test_f1\"])))\n",
    "print(\"Mean Precision Score: {:.3f}\".format(np.mean(lgs_scores[\"test_precision\"])))\n",
    "print(\"Mean Recall Score: {:.3f}\".format(np.mean(lgs_scores[\"test_recall\"])))\n",
    "print(\"Mean Accuracy Score: {:.3f}\".format(np.mean(lgs_scores[\"test_accuracy\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using word2vec inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T06:47:26.202598Z",
     "start_time": "2021-01-09T06:47:23.113980Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dongwook/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dongwook/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/dongwook/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean F1 Score: 0.880\n",
      "Mean Precision Score: 0.876\n",
      "Mean Recall Score: 0.884\n",
      "Mean Accuracy Score: 0.880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dongwook/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model by 5 fold cross validation\n",
    "scoring = ['f1','precision','recall', \"accuracy\"]    # metrics to be evaluated\n",
    "lgs_scores = cross_validate(lgs, X_word2vec, y, scoring=scoring, cv=5, return_train_score=False)\n",
    "\n",
    "print(\"Mean F1 Score: {:.3f}\".format(np.mean(lgs_scores[\"test_f1\"])))\n",
    "print(\"Mean Precision Score: {:.3f}\".format(np.mean(lgs_scores[\"test_precision\"])))\n",
    "print(\"Mean Recall Score: {:.3f}\".format(np.mean(lgs_scores[\"test_recall\"])))\n",
    "print(\"Mean Accuracy Score: {:.3f}\".format(np.mean(lgs_scores[\"test_accuracy\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It seems like by using logistic regression, word2vec vectorization fits the best by showing about 88% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "- This time, we use random forest model which is a bit more advanced than logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T06:47:26.291279Z",
     "start_time": "2021-01-09T06:47:26.204430Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Random forest classifier with no hyperparameters tuned\n",
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TF-IDF inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T06:47:54.693017Z",
     "start_time": "2021-01-09T06:47:26.292877Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean F1 Score: 0.814\n",
      "Mean Precision Score: 0.807\n",
      "Mean Recall Score: 0.821\n",
      "Mean Accuracy Score: 0.812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:   28.3s finished\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model by 3 fold cross validation\n",
    "scoring = ['f1','precision','recall', \"accuracy\"]    # metrics to be evaluated\n",
    "rf_scores = cross_validate(rf, X_TFIDF, y, scoring=scoring, cv=3, return_train_score=False, \n",
    "                           verbose=1, n_jobs=-1)\n",
    "\n",
    "print(\"Mean F1 Score: {:.3f}\".format(np.mean(rf_scores[\"test_f1\"])))\n",
    "print(\"Mean Precision Score: {:.3f}\".format(np.mean(rf_scores[\"test_precision\"])))\n",
    "print(\"Mean Recall Score: {:.3f}\".format(np.mean(rf_scores[\"test_recall\"])))\n",
    "print(\"Mean Accuracy Score: {:.3f}\".format(np.mean(rf_scores[\"test_accuracy\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Count Vectorizer inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T06:48:23.199179Z",
     "start_time": "2021-01-09T06:47:54.697978Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean F1 Score: 0.808\n",
      "Mean Precision Score: 0.803\n",
      "Mean Recall Score: 0.812\n",
      "Mean Accuracy Score: 0.806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:   28.4s finished\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model by 3 fold cross validation\n",
    "scoring = ['f1','precision','recall', \"accuracy\"]    # metrics to be evaluated\n",
    "rf_scores = cross_validate(rf, X_countvect, y, scoring=scoring, cv=3, return_train_score=False,\n",
    "                          verbose=1, n_jobs=-1)\n",
    "\n",
    "print(\"Mean F1 Score: {:.3f}\".format(np.mean(rf_scores[\"test_f1\"])))\n",
    "print(\"Mean Precision Score: {:.3f}\".format(np.mean(rf_scores[\"test_precision\"])))\n",
    "print(\"Mean Recall Score: {:.3f}\".format(np.mean(rf_scores[\"test_recall\"])))\n",
    "print(\"Mean Accuracy Score: {:.3f}\".format(np.mean(rf_scores[\"test_accuracy\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using word2vec inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T06:49:42.303425Z",
     "start_time": "2021-01-09T06:48:23.203013Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean F1 Score: 0.851\n",
      "Mean Precision Score: 0.831\n",
      "Mean Recall Score: 0.873\n",
      "Mean Accuracy Score: 0.848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  1.3min finished\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model by 3 fold cross validation\n",
    "scoring = ['f1','precision','recall', \"accuracy\"]    # metrics to be evaluated\n",
    "rf_scores = cross_validate(rf, X_word2vec, y, scoring=scoring, cv=3, return_train_score=False,\n",
    "                          verbose=1, n_jobs=1)\n",
    "\n",
    "print(\"Mean F1 Score: {:.3f}\".format(np.mean(rf_scores[\"test_f1\"])))\n",
    "print(\"Mean Precision Score: {:.3f}\".format(np.mean(rf_scores[\"test_precision\"])))\n",
    "print(\"Mean Recall Score: {:.3f}\".format(np.mean(rf_scores[\"test_recall\"])))\n",
    "print(\"Mean Accuracy Score: {:.3f}\".format(np.mean(rf_scores[\"test_accuracy\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For all input vectors random forest was beyond performance than logistic regression.\n",
    "- This implies that using advanced model doesn't necessarily guarentee better performace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T06:49:43.154882Z",
     "start_time": "2021-01-09T06:49:42.305230Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-09 15:49:42,741 : INFO : NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# XGBoost Classifier without hyperparameters tuned\n",
    "XGB = XGBClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TF-IDF inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T06:49:59.550602Z",
     "start_time": "2021-01-09T06:49:43.157010Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean F1 Score: 0.818\n",
      "Mean Precision Score: 0.803\n",
      "Mean Recall Score: 0.833\n",
      "Mean Accuracy Score: 0.814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:   16.3s finished\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model by 3 fold cross validation\n",
    "scoring = ['f1','precision','recall', \"accuracy\"]    # metrics to be evaluated\n",
    "xgb_scores = cross_validate(XGB, X_TFIDF, y, scoring=scoring, cv=3, return_train_score=False, \n",
    "                            verbose=1, n_jobs=-1)\n",
    "\n",
    "print(\"Mean F1 Score: {:.3f}\".format(np.mean(xgb_scores[\"test_f1\"])))\n",
    "print(\"Mean Precision Score: {:.3f}\".format(np.mean(xgb_scores[\"test_precision\"])))\n",
    "print(\"Mean Recall Score: {:.3f}\".format(np.mean(xgb_scores[\"test_recall\"])))\n",
    "print(\"Mean Accuracy Score: {:.3f}\".format(np.mean(xgb_scores[\"test_accuracy\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USing Count Vectorizer inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T06:50:07.484809Z",
     "start_time": "2021-01-09T06:49:59.554576Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean F1 Score: 0.823\n",
      "Mean Precision Score: 0.807\n",
      "Mean Recall Score: 0.839\n",
      "Mean Accuracy Score: 0.819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    7.8s finished\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model by 3 fold cross validation\n",
    "scoring = ['f1','precision','recall', \"accuracy\"]    # metrics to be evaluated\n",
    "xgb_scores = cross_validate(XGB, X_countvect, y, scoring=scoring, cv=3, return_train_score=False, \n",
    "                            verbose=1, n_jobs=-1)\n",
    "\n",
    "print(\"Mean F1 Score: {:.3f}\".format(np.mean(xgb_scores[\"test_f1\"])))\n",
    "print(\"Mean Precision Score: {:.3f}\".format(np.mean(xgb_scores[\"test_precision\"])))\n",
    "print(\"Mean Recall Score: {:.3f}\".format(np.mean(xgb_scores[\"test_recall\"])))\n",
    "print(\"Mean Accuracy Score: {:.3f}\".format(np.mean(xgb_scores[\"test_accuracy\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using word2vec inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T06:51:46.468079Z",
     "start_time": "2021-01-09T06:50:07.488397Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean F1 Score: 0.868\n",
      "Mean Precision Score: 0.862\n",
      "Mean Recall Score: 0.874\n",
      "Mean Accuracy Score: 0.867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:  1.6min finished\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model by 3 fold cross validation\n",
    "scoring = ['f1','precision','recall', \"accuracy\"]    # metrics to be evaluated\n",
    "xgb_scores = cross_validate(XGB, X_word2vec, y, scoring=scoring, cv=3, return_train_score=False, \n",
    "                            verbose=1, n_jobs=-1)\n",
    "\n",
    "print(\"Mean F1 Score: {:.3f}\".format(np.mean(xgb_scores[\"test_f1\"])))\n",
    "print(\"Mean Precision Score: {:.3f}\".format(np.mean(xgb_scores[\"test_precision\"])))\n",
    "print(\"Mean Recall Score: {:.3f}\".format(np.mean(xgb_scores[\"test_recall\"])))\n",
    "print(\"Mean Accuracy Score: {:.3f}\".format(np.mean(xgb_scores[\"test_accuracy\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "- Considering the results of above models, we could see that validation accuracy didn't improve from 88%.\n",
    "- This seems like the best we can do with our preprocessed data, so if we want to increase accuracy, we have to go back to the preprocessing steps and try to extract more features of words.\n",
    "- However, out of expectation that deep learning models might be able to perform better than machine learning models, we will try to fit our data to RNN and CNN."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
