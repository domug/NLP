{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "\n",
    "- RNN takes account the order of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T07:18:28.980970Z",
     "start_time": "2021-01-10T07:18:24.331666Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "SEED_NUM = 7\n",
    "tf.random.set_seed(SEED_NUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import preprocessed data\n",
    "- RNN takes tokenized word vectors as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T07:18:29.889036Z",
     "start_time": "2021-01-10T07:18:29.885756Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T07:18:31.399110Z",
     "start_time": "2021-01-10T07:18:31.375971Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[26113,   122,     1, ..., 18720,   317,  1358],\n",
       "       [  234,   206,  3051, ...,     0,     0,     0],\n",
       "       [ 6053,  4960,   460, ...,   702,  1190,  5314],\n",
       "       ...,\n",
       "       [  119,  3108,    16, ...,     0,     0,     0],\n",
       "       [  640,   518, 16618, ...,     0,     0,     0],\n",
       "       [  111,     1,   350, ...,     0,     0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train inputs\n",
    "train_input = np.load(open(\"Preprocessed/train_inputs.npy\", \"rb\"))\n",
    "train_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T07:18:32.247572Z",
     "start_time": "2021-01-10T07:18:32.240187Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train labels\n",
    "train_label = np.load(open(\"Preprocessed/train_labels.npy\", \"rb\"))\n",
    "train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T07:18:32.911494Z",
     "start_time": "2021-01-10T07:18:32.866624Z"
    }
   },
   "outputs": [],
   "source": [
    "# word dictionary\n",
    "prepo_configs = json.load(open(\"Preprocessed/data_configs.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T07:18:34.176582Z",
     "start_time": "2021-01-10T07:18:34.171675Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters for model training\n",
    "model_name = \"rnn_classifier_english\"\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 5\n",
    "VALID_SPLIT = 0.2\n",
    "\n",
    "\n",
    "# Hyperparameters for model layers\n",
    "kwargs = {\"model_name\": model_name,\n",
    "        \"vocab_size\": prepo_configs[\"vocab_size\"],\n",
    "        \"embedding_dimension\": 100,\n",
    "        \"dropout_rate\": 0.2,\n",
    "        \"lstm_dimension\": 150,\n",
    "        \"dense_dimension\": 150,\n",
    "        \"output_dimension\": 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model Structure\n",
    "\n",
    "- Define structure of RNN by class method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T07:18:35.804758Z",
     "start_time": "2021-01-10T07:18:35.795966Z"
    }
   },
   "outputs": [],
   "source": [
    "class RNNClassifier(tf.keras.Model):\n",
    "    \n",
    "    # __init__ method\n",
    "    def __init__(self, **kwargs):\n",
    "        super(RNNClassifier, self).__init__(name = kwargs[\"model_name\"])  # super() is used to inherit parent's variables \n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim = kwargs[\"vocab_size\"], \n",
    "                                          output_dim = kwargs[\"embedding_dimension\"])\n",
    "        self.lstm_1_layer = tf.keras.layers.LSTM(kwargs[\"lstm_dimension\"], return_sequences = True)\n",
    "        self.lstm_2_layer = tf.keras.layers.LSTM(kwargs[\"lstm_dimension\"])\n",
    "        self.dropout = tf.keras.layers.Dropout(kwargs[\"dropout_rate\"])\n",
    "        self.fc1 = tf.keras.layers.Dense(units = kwargs[\"dense_dimension\"], \n",
    "                                         activation = tf.keras.activations.tanh)\n",
    "        self.fc2 = tf.keras.layers.Dense(units = kwargs[\"output_dimension\"], \n",
    "                                         activation = tf.keras.activations.sigmoid)\n",
    "     \n",
    "    \n",
    "    # call method to run layers\n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)      # word embedding layer\n",
    "        x = self.dropout(x)        # dropout layer to prevent overfitting\n",
    "        x = self.lstm_1_layer(x)   # pass first LSTM layer\n",
    "        x = self.lstm_2_layer(x)   # pass second LSTM lsyer\n",
    "        x = self.dropout(x)        # another dropout layer\n",
    "        x = self.fc1(x)            # fully connected layer with tanh activation\n",
    "        x = self.fc2(x)            # output layer (binary classification)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Compiling\n",
    "\n",
    "- We are going to use Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T07:18:37.401515Z",
     "start_time": "2021-01-10T07:18:37.347950Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create rnn model\n",
    "rnn = RNNClassifier(**kwargs)  \n",
    "\n",
    "# Comile model\n",
    "rnn.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0001),  # optimizer\n",
    "           loss = tf.keras.losses.BinaryCrossentropy(),                    # loss function\n",
    "           metrics = [tf.keras.metrics.BinaryAccuracy(name=\"accuracy\")])   # evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "---\n",
    "**EarlyStopping** is used to prevent overfitting.\n",
    "- `monitor`: validating score\n",
    "- `min_delta`: the threshold that triggers the termination (acc should at least improve by the set value)\n",
    "- `patience`: no improvement epochs (session gets terminated if acc doesn't improve during the set epochs)\n",
    "\n",
    "--- \n",
    "**ModelCheckpoint** saves model for each epochs.\n",
    "- `save_best_only`: only save the best performing model\n",
    "- `save_weights_only`: only save model weights instead of entire model graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T04:05:49.369966Z",
     "start_time": "2021-01-10T04:03:36.423399Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# EarlyStopping\n",
    "earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor = \"val_accuracy\", min_delta = 0.0001, patience = 2)\n",
    "\n",
    "\n",
    "# ModelCheckpoint\n",
    "SAVE_PATH = \"Trained_model/\"\n",
    "checkpoint_path = SAVE_PATH + model_name + \"/weight.h5\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
    "else:\n",
    "    os.makedirs(checkpoint_dir, exist_ok = True)\n",
    "    print(\"{} -- Folder newly created \\n\".format(checkpoint_dir))\n",
    "    \n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, monitor=\"val_accuracy\", \n",
    "                                   verbose=1, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history = rnn.fit(train_input, train_label, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,\n",
    "                 validation_split=VALID_SPLIT, callbacks=[earlystop_callback, cp_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since it takes a while to train rnn, I used GPU from Google Colab.\n",
    "- We will skip the training process and load the trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T07:19:17.949569Z",
     "start_time": "2021-01-10T07:18:40.984550Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 36s 46ms/step - loss: 0.6931 - accuracy: 0.5047\n"
     ]
    }
   ],
   "source": [
    "rnn.evaluate(train_input, train_label)   # Initialize RNN\n",
    "rnn.load_weights(\"IMDB_rnn.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T07:19:54.483993Z",
     "start_time": "2021-01-10T07:19:19.398321Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 - 35s - loss: 0.1873 - accuracy: 0.9328\n",
      "Accuracy of recovered model: 93.28%\n"
     ]
    }
   ],
   "source": [
    "loss, acc = rnn.evaluate(train_input, train_label, verbose=2)\n",
    "print(\"Accuracy of recovered model: {:5.2f}%\".format(100*acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "\n",
    "- CNN takes account the regional information of sentences\n",
    "- We are using 3 Conv1D filters (size of 3, 4, 5 respectively) and Maxpooling for each to extract representative features of sentence\n",
    "- `MaxNorm` argument indicates a regularization on the magnitude of weight vectors to prevent exploding gradient problem. Typical values are 3 and 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T07:20:21.364585Z",
     "start_time": "2021-01-10T07:20:21.359192Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"cnn_classifier_en\"\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 10\n",
    "VALID_SPLIT = 0.1\n",
    "MAX_LEN = train_input.shape[1]\n",
    "\n",
    "kwargs = {\n",
    "    \"model_name\": model_name,\n",
    "    \"vocab_size\": prepo_configs[\"vocab_size\"],\n",
    "    \"embedding_size\": 128,\n",
    "    \"num_filters\": 100,\n",
    "    \"dropout_rate\": 0.5,\n",
    "    \"hidden_dimension\": 250,\n",
    "    \"output_dimension\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model Structure\n",
    "- Similar with RNN modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T07:20:22.820649Z",
     "start_time": "2021-01-10T07:20:22.809541Z"
    }
   },
   "outputs": [],
   "source": [
    "class CNNClassifier(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(CNNClassifier, self).__init__(name=kwargs[\"model_name\"])\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=kwargs[\"vocab_size\"],\n",
    "                                                  output_dim=kwargs[\"embedding_size\"])\n",
    "        self.conv_list = [tf.keras.layers.Conv1D(filters=kwargs[\"num_filters\"],\n",
    "                                                kernel_size=kernel_size,\n",
    "                                                padding=\"valid\",\n",
    "                                                activation=tf.keras.activations.swish,\n",
    "                                                kernel_constraint=tf.keras.constraints.MaxNorm(max_value=3.))\n",
    "                         for kernel_size in [3,4,5]]\n",
    "        self.pooling = tf.keras.layers.GlobalMaxPooling1D()\n",
    "        self.dropout = tf.keras.layers.Dropout(kwargs[\"dropout_rate\"])\n",
    "        self.fc1 = tf.keras.layers.Dense(units=kwargs[\"hidden_dimension\"],\n",
    "                                        activation=tf.keras.activations.swish,\n",
    "                                        kernel_constraint=tf.keras.constraints.MaxNorm(max_value=3.))\n",
    "        self.fc2 = tf.keras.layers.Dense(units=kwargs[\"output_dimension\"],\n",
    "                                        activation=tf.keras.activations.sigmoid,\n",
    "                                        kernel_constraint=tf.keras.constraints.MaxNorm(max_value=3.))\n",
    "        \n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        x = tf.concat([self.pooling(conv(x)) for conv in self.conv_list], axis=1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T07:20:25.945538Z",
     "start_time": "2021-01-10T07:20:25.914475Z"
    }
   },
   "outputs": [],
   "source": [
    "cnn = CNNClassifier(**kwargs)\n",
    "cnn.compile(optimizer=tf.keras.optimizers.Adam(0.0001),\n",
    "             loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "             metrics=[tf.keras.metrics.BinaryAccuracy(name=\"accuracy\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "- We use EarlyStopping and Model Checkpoint technic as same as we did for RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# EarlyStopping\n",
    "earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor = \"val_accuracy\", min_delta = 0.0001, patience = 2)\n",
    "\n",
    "\n",
    "# ModelCheckpoint\n",
    "SAVE_PATH = \"Trained_model/\"\n",
    "checkpoint_path = SAVE_PATH + model_name + \"/weight.h5\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
    "else:\n",
    "    os.makedirs(checkpoint_dir, exist_ok = True)\n",
    "    print(\"{} -- Folder newly created \\n\".format(checkpoint_dir))\n",
    "    \n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path, monitor=\"val_accuracy\", verbose=1, save_best_only=True, save_weights_only=False\n",
    ")\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history = cnn.fit(train_input, train_label, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,\n",
    "                 validation_split=VALID_SPLIT, callbacks=[earlystop_callback, cp_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Same as RNN, training was done in Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T07:20:34.322945Z",
     "start_time": "2021-01-10T07:20:28.177972Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 6s 7ms/step - loss: 0.6933 - accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "cnn.evaluate(train_input, train_label)   # Initialize CNN\n",
    "cnn.load_weights(\"IMDB_cnn.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-10T07:20:40.799890Z",
     "start_time": "2021-01-10T07:20:34.324840Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 - 6s - loss: 0.0773 - accuracy: 0.9869\n",
      "Accuracy of recovered model: 98.69%\n"
     ]
    }
   ],
   "source": [
    "loss, acc = cnn.evaluate(train_input, train_label, verbose=2)\n",
    "print(\"Accuracy of recovered model: {:5.2f}%\".format(100*acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "- It certainly seems like DL models perform better than ML models for movie review dataset, but keep it mind that it's not always.\n",
    "- Roughly speaking, it is said that DL models perform better as the size of data is sufficiently large."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
